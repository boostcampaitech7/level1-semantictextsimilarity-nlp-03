{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/pickle/new.pickle', 'rb') as file:\n",
    "    dict_data = pickle.load(file)\n",
    "\n",
    "with open('data/pickle/new_pickle.txt', 'w') as f:\n",
    "    f.write(str(dict_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 글자 key 삭제\n",
    "dict_data = {key: value for key, value in dict_data.items() if len(key) > 1}\n",
    "\n",
    "with open('data/pickle/new.pickle', 'wb') as file:\n",
    "    pickle.dump(dict_data, file)\n",
    "\n",
    "with open('data/pickle/new.pickle', 'rb') as file:\n",
    "    dict_data = pickle.load(file)\n",
    "\n",
    "with open('data/pickle/new_pickle.txt', 'w') as f:\n",
    "    f.write(str(dict_data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 제거\n",
    "key_to_remove = '틀림'\n",
    "if key_to_remove in dict_data:\n",
    "    del dict_data[key_to_remove]\n",
    "\n",
    "with open('data/pickle/new.pickle', 'wb') as file:\n",
    "    pickle.dump(dict_data, file)\n",
    "\n",
    "with open('data/pickle/new.pickle', 'rb') as file:\n",
    "    dict_data = pickle.load(file)\n",
    "\n",
    "with open('data/pickle/new_pickle.txt', 'w') as f:\n",
    "    f.write(str(dict_data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value 제거\n",
    "key = '시간'\n",
    "value_to_remove = '성상'\n",
    "\n",
    "if key in dict_data:\n",
    "    dict_data[key] = [v for v in dict_data[key] if v != value_to_remove]\n",
    "\n",
    "with open('data/pickle/new.pickle', 'wb') as file:\n",
    "    pickle.dump(dict_data, file)\n",
    "\n",
    "with open('data/pickle/new.pickle', 'rb') as file:\n",
    "    dict_data = pickle.load(file)\n",
    "\n",
    "with open('data/pickle/new_pickle.txt', 'w') as f:\n",
    "    f.write(str(dict_data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 추가\n",
    "dict_data['일단'] = ['우선, 먼저']\n",
    "\n",
    "with open('data/pickle/new.pickle', 'wb') as file:\n",
    "    pickle.dump(dict_data, file)\n",
    "\n",
    "with open('data/pickle/new.pickle', 'rb') as file:\n",
    "    dict_data = pickle.load(file)\n",
    "\n",
    "with open('data/pickle/new_pickle.txt', 'w') as f:\n",
    "    f.write(str(dict_data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "with open('data/pickle/new.pickle', 'rb') as file:\n",
    "    dict_data = pickle.load(file)\n",
    "\n",
    "def augment_sentence(sentence, dict_data):\n",
    "    if not isinstance(sentence, str):\n",
    "        return []\n",
    "    \n",
    "    words = sentence.split()\n",
    "    new_sentences = []\n",
    "    sentence_changed = False\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in dict_data:\n",
    "            synonyms = [syn for syn in dict_data[word] if syn != word]\n",
    "            if synonyms:\n",
    "                sentence_changed = True\n",
    "                for synonym in synonyms:\n",
    "                    new_sentence = words[:i] + [synonym] + words[i+1:]\n",
    "                    new_sentences.append(' '.join(new_sentence))\n",
    "    \n",
    "    if sentence_changed:\n",
    "        return new_sentences\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def generate_augmented_sentences(df, dict_data, labels, num_aug_per_label):\n",
    "    augmented_rows = []\n",
    "    test_rows = []\n",
    "\n",
    "    for label, num_augmentations in zip(labels, num_aug_per_label):\n",
    "        label_df = df[df['label'] == label]\n",
    "\n",
    "        for _, row in label_df.iterrows():\n",
    "            for column in ['sentence_1', 'sentence_2']:\n",
    "                original_sentence = row[column]\n",
    "                new_sentences = augment_sentence(original_sentence, dict_data)\n",
    "\n",
    "                if new_sentences:\n",
    "                    for new_sentence in new_sentences:\n",
    "                        augmented_row = row.copy()\n",
    "                        augmented_row[column] = new_sentence\n",
    "                        augmented_row['source'] = f\"{row['source']}-sr\" \n",
    "                        augmented_rows.append(augmented_row)\n",
    "                        \n",
    "                        test_rows.append({\n",
    "                            'original_sentence': original_sentence,\n",
    "                            'augmented_sentence': new_sentence\n",
    "                        })\n",
    "\n",
    "                if len(augmented_rows) >= num_augmentations:\n",
    "                    break\n",
    "            \n",
    "            if len(augmented_rows) >= num_augmentations:\n",
    "                break\n",
    "\n",
    "        while len(augmented_rows) < num_augmentations:\n",
    "            random_row = label_df.sample(1).iloc[0]\n",
    "            random_column = random.choice(['sentence_1', 'sentence_2'])\n",
    "            random_sentence = random_row[random_column]\n",
    "            new_sentences = augment_sentence(random_sentence, dict_data)\n",
    "\n",
    "            if new_sentences:\n",
    "                for new_sentence in new_sentences:\n",
    "                    augmented_row = random_row.copy()\n",
    "                    augmented_row[random_column] = new_sentence\n",
    "                    augmented_row['source'] = f\"{random_row['source']}-sr\" \n",
    "                    augmented_rows.append(augmented_row)\n",
    "                    \n",
    "                    test_rows.append({\n",
    "                        'original_sentence': random_sentence,\n",
    "                        'augmented_sentence': new_sentence\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(augmented_rows), pd.DataFrame(test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "input_file = 'data/train.csv'  # 원본 파일 경로\n",
    "test_file = 'data/sr_test.csv'  # 테스트 파일 경로\n",
    "output_file = 'data/train_sr.csv'  # 저장 파일 경로\n",
    "\n",
    "labels = [1.5, 2.5, 4.5]  # 증강이 필요한 라벨\n",
    "num_aug_per_label = [504, 512, 798]  # 필요한 증강 개수\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_file)\n",
    "\n",
    "augmented_df, test_df = generate_augmented_sentences(df, dict_data, labels, num_aug_per_label)\n",
    "\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "test_df.to_csv(test_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
